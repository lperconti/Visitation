{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Load the dataset, take a look at the data info: \n",
    "df_1 = pd.read_csv('Data/DF_1_clean.csv')\n",
    "df_1.set_index('Date', inplace=True)\n",
    "df_2 = pd.read_csv('Data/DF_2_clean.csv')\n",
    "df_2.set_index('Date', inplace=True)\n",
    "df_3 = pd.read_csv('Data/DF_3_clean.csv')\n",
    "df_3.set_index('Date', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resort 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First Simple Model\n",
    "Linear Regression with Cross Validation: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Split into test and train X and Y\n",
    "X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "y = df_1['TOTAL_VISITS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#visualization of target - Visits\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(y, bins=100);\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Split into test and train X and Y\n",
    "# X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "# y = df_1['TOTAL_VISITS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t('scaler', StandardScaler(), non_cat),\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#setting up the pipe and training the model: \n",
    "\n",
    "LRPipe = Pipeline([('trans', Transformer),\n",
    "                ('linreg', LinearRegression())])\n",
    "\n",
    "LRPipe.fit(X_train, y_train)\n",
    "\n",
    "#cross validation and scoring\n",
    "cv = cross_validate(LRPipe, X_train, y_train, return_train_score=True, cv=3)\n",
    "cv_score = cross_val_score(LRPipe, X_train, y_train, cv = 3)\n",
    "\n",
    "print(f'Cross Validation Scores: {cv}')\n",
    "\n",
    "print(f'mean CV Score: {cv_score.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Encoding and scaling, the Basic Linear Regression CV score is 0.576\n",
    "\n",
    "There is definitly some room for improvement! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tree Regressor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Split into test and train X and Y\n",
    "X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "y = df_1['TOTAL_VISITS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "TreeRegPipe = Pipeline([('trans', Transformer),\n",
    "                        ('dtr', dtr)])\n",
    "\n",
    "dtr_param_grid = {\n",
    "\t'dtr__splitter': ['best', 'random'],\n",
    "    'dtr__min_samples_split': [4, 6, 8, 10],\n",
    "    'dtr__max_depth': [4, 5, 6, 7, 8]\n",
    "}\t\t\n",
    "\n",
    "#set up gridsearch:\n",
    "TreeGrid = GridSearchCV(estimator=TreeRegPipe,\n",
    "\t\t\t\t\t\t  param_grid=dtr_param_grid,\n",
    "\t\t\t\t\t\t  cv=3)\n",
    "\n",
    "TreeGrid.fit(X_train, y_train)\n",
    "\n",
    "#cross validation and scoring\n",
    "cv = cross_validate(TreeGrid, X_train, y_train, return_train_score=True, cv=3)\n",
    "cv_score = cross_val_score(TreeGrid, X_train, y_train, cv = 3)\n",
    "\n",
    "print(\"Best Parameter Combination for Tree Regressor Found During Grid Search:\")\n",
    "print(TreeGrid.best_params_)\n",
    "\n",
    "print(f'Tree Grid Cross Validation Scores: {cv}')\n",
    "\n",
    "print(f'Tree Grid mean CV Score: {cv_score.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tree Regressor did not perform well with the best score of .278 in comparison to linear regression with a score of .576"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forests "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pipeline_3:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# #Split into test and train X and Y\n",
    "# X = df_3_preprocessed.drop(columns=['TOTAL_VISITS'])\n",
    "# y = df_3_preprocessed['TOTAL_VISITS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month]\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "rfr = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_3 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('dtr', rfr)])\n",
    "\n",
    "#set up grid search paramater grid\n",
    "rfr_param_grid = {\n",
    "    'dtr__n_estimators': [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "    'dtr__max_depth': [None, 2, 3, 4, 5, 6, 8, 10],\n",
    "\t'dtr__max_features': ['auto', 'sqrt', 'log2'],\n",
    "\t'dtr__criterion': ['mse', 'mae']\n",
    "}\n",
    "\n",
    "#set up gridsearch:\n",
    "gridsearch2 = GridSearchCV(estimator=pipeline_3,\n",
    "\t\t\t\t\t\t  param_grid=rfr_param_grid,\n",
    "\t\t\t\t\t\t  cv=3)\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "print(gridsearch2.best_params_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pipeline_3:\n",
    "\n",
    "#Split into test and train X and Y\n",
    "X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "y = df_1['TOTAL_VISITS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "rfr = RandomForestRegressor(criterion = 'mse', max_depth = None, max_features = 'sqrt', n_estimators = 10, random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_3 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('dtr', rfr)])\n",
    "\n",
    "\n",
    "pipeline_3.fit(X_train, y_train)\n",
    "\n",
    "cv = cross_validate(pipeline_3, X_train, y_train, return_train_score=True, cv=5)\n",
    "cv_score = cross_val_score(pipeline_3, X_train, y_train, cv = 5)\n",
    "print(f'Cross Validation Scores best Random Forest Regressor: {cv}')\n",
    "\n",
    "print(f'mean CV Score: {cv_score.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Resort 1, forests are performing better than trees with a CV score of .638!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "#pipeline_4:\n",
    "\n",
    "#Split into test and train X and Y\n",
    "# X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "# y = df_1['TOTAL_VISITS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "GBR = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_4 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('gbr', GBR)])\n",
    "\n",
    "#set up grid search paramater grid\n",
    "gbr_param_grid = {\n",
    "    'gbr__n_estimators': [50, 100, 200, 300],\n",
    "    'gbr__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "\t'gbr__min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "#set up gridsearch:\n",
    "gridsearch3 = GridSearchCV(estimator=pipeline_4,\n",
    "\t\t\t\t\t\t  param_grid=gbr_param_grid,\n",
    "\t\t\t\t\t\t  cv=5)\n",
    "\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameter Combination Found During Gradient Boosting Grid Search:\")\n",
    "print(gridsearch3.best_params_)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Using best params on a gradient booster regressor then scoring thorugh cross val: \n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "# GBR = GradientBoostingRegressor(criterion='mse', n_estimators=200, random_state = 42)\n",
    "GBR = GradientBoostingRegressor(criterion='mse', n_estimators=200, min_samples_split= 5, random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_gbr = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('gbr', GBR)])\n",
    "\n",
    "pipeline_gbr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "cv = cross_validate(pipeline_gbr, X_train, y_train, return_train_score=True, cv=5)\n",
    "cv_score = cross_val_score(pipeline_gbr, X_train, y_train, cv = 5)\n",
    "print(f'Cross Validation Score Gradient Boosting Model: {cv}')\n",
    "\n",
    "print(f'mean CV Score: {cv_score.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Booster Regressor is doing the best: \n",
    "\n",
    "And more improvement! We got our average model CV score up to .691"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xg\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#x_boost Pipe:\n",
    "\n",
    "# X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "# y = df_1['TOTAL_VISITS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline first step: Transformer\n",
    "\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t('scaler', StandardScaler(), non_cat),\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "xgb_r = xg.XGBRegressor(seed = 123)\n",
    "\n",
    "#pipeline\n",
    "xboostPipe = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('xgb_r', xgb_r)])\n",
    "\n",
    "# objective ='reg:squarederror', n_estimators = 10, \n",
    "boost_param_grid = {\n",
    "    'xgb_r__eta': [.2, .3, .4, .5, .6, .7, .8],\n",
    "\t'xgb_r__max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "\n",
    "gridsearch_boost = GridSearchCV(estimator=xboostPipe,\n",
    "\t\t\t\t\t\t  param_grid=boost_param_grid,\n",
    "\t\t\t\t\t\t  cv=5)\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch_boost.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "print(gridsearch_boost.best_params_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#x_boost Pipe with ideal paramaters:\n",
    "\n",
    "# X = df_1.drop(columns=['TOTAL_VISITS'])\n",
    "# y = df_1['TOTAL_VISITS']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline first step: Transformer\n",
    "# cat = ['is_school_out', 'is_holiday', 'day_of_week_num', 'month']\n",
    "# non_cat = ['T - AWND', 'T - PRCP', 'T - SNOW', 'T - SNWD', 'T - TAVG', 'M_PRCP', 'M_SNWD', 'M_TAVG']\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t('scaler', StandardScaler(), non_cat),\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: \n",
    "xgb_r_2 = xg.XGBRegressor(eta = 0.2, max_depth = 3, seed = 123)\n",
    "\n",
    "xboostPipe_2 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('xgb_r_2', xgb_r_2)])\n",
    "\n",
    "xboostPipe_2.fit(X_train, y_train)\n",
    "\n",
    "cv = cross_validate(xboostPipe_2, X_train, y_train, return_train_score=True, cv=5)\n",
    "cv_score = cross_val_score(xboostPipe_2, X_train, y_train, cv = 5)\n",
    "print(f'Cross Validation Scores: {cv}')\n",
    "\n",
    "print(f'mean CV Score: {cv_score.mean()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "X boost for resort 1 is preforming slightly better than Gradient Booster Regressor at CV score of .701"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "**XBoost** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#GBR Test Score\n",
    "xboostPipe_2.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "XBoost gets a test CV score of .782, this means we can account for 78% of the variance in the Resort 1 data with our Gradient Booster Regressor model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred_xboost = xboostPipe_2.predict(X_test)\n",
    "\n",
    "df = pd.DataFrame({'Real Visits Resort 1':y_test, 'Predicted XBoost Visits':y_pred_xboost, \n",
    "                    'Difference': abs(y_test - y_pred_xboost)})\n",
    "df.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Difference'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ohe = xboostPipe_2.named_steps[\"transformer\"].transformers_[0][1].get_feature_names()\n",
    "numbers = non_cat\n",
    "\n",
    "columns = [*ohe, *numbers]\n",
    "\n",
    "for name, importance in zip(columns, xboostPipe_2['xgb_r_2'].feature_importances_):\n",
    "    print(name, importance)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "feature_importance = xboostPipe_2['xgb_r_2'].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0])\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(columns)[sorted_idx])\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    xboostPipe_2, X_test, y_test, n_repeats=19, random_state=42, n_jobs=2\n",
    ")\n",
    "result\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(columns)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resort 2 Modeling\n",
    "\n",
    "## Resort 2 Base Model\n",
    "\n",
    "Start with basic Linear Regression with OHE and standard scaler for our Resort 2 Base Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Split into test and train X and Y - labled 2 for resort 2\n",
    "X2 = df_2.drop(columns=['TOTAL_VISITS'])\n",
    "y2 = df_2['TOTAL_VISITS']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_2.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 468 entries, 2015-12-14 to 2021-04-18\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   IS_SCHOOL_OUT    468 non-null    int64  \n",
      " 1   IS_HOLIDAY       468 non-null    int64  \n",
      " 2   TOTAL_VISITS     468 non-null    int64  \n",
      " 3   M_TEMPAVERAGE_F  468 non-null    int64  \n",
      " 4   M_PRECIP_INCHES  468 non-null    float64\n",
      " 5   T_PRCP           468 non-null    float64\n",
      " 6   T_TAVG           468 non-null    float64\n",
      " 7   day_of_week_num  468 non-null    int64  \n",
      " 8   Month            468 non-null    int64  \n",
      "dtypes: float64(3), int64(6)\n",
      "memory usage: 56.6+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#visualization of target - Visits\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(y2, bins=100);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# #Split into test and train X and Y - labled 2 for resort 2\n",
    "# X2 = df_2.drop(columns=['TOTAL_VISITS'])\n",
    "# y2 = df_2['TOTAL_VISITS']\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "cat = ['IS_HOLIDAY', 'day_of_week_num', 'Month', 'IS_SCHOOL_OUT']\n",
    "non_cat = ['M_TEMPAVERAGE_F', 'M_PRECIP_INCHES', 'T_PRCP', 'T_TAVG',]\n",
    "\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t('scaler', StandardScaler(), non_cat),\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#setting up the pipe and training the model: \n",
    "\n",
    "LR_resort2 = LinearRegression()\n",
    "\n",
    "LRPipe_resort2 = Pipeline([('trans', Transformer),\n",
    "                ('linreg_resort2', LR_resort2)])\n",
    "\n",
    "LRPipe_resort2.fit(X_train2, y_train2)\n",
    "\n",
    "#cross validation and scoring\n",
    "lr_cv_resort2 = cross_validate(LRPipe_resort2, X_train2, y_train2, return_train_score=True, cv=5)\n",
    "lr_cv_score_resort2 = cross_val_score(LRPipe_resort2, X_train2, y_train2, cv = 5)\n",
    "\n",
    "print(f'Cross Validation Scores, Linear Regression, Resort 2: {lr_cv_resort2}')\n",
    "\n",
    "print(f'mean CV Score, Linear Regression, Resort 2: {lr_cv_score_resort2.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Scores, Linear Regression, Resort 2: {'fit_time': array([0.01873899, 0.02753568, 0.03098607, 0.01676989, 0.01431894]), 'score_time': array([0.00949883, 0.01742005, 0.01185203, 0.01299691, 0.01266909]), 'test_score': array([0.28257942, 0.4323264 , 0.29244525, 0.4724927 , 0.38015785]), 'train_score': array([0.46812703, 0.43163345, 0.46853585, 0.41525936, 0.43788248])}\n",
      "mean CV Score, Linear Regression, Resort 2: 0.37200032562717567\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Resort 2, Basic Linear Regression CV score is .372\n",
    "\n",
    "Because Tree Regressors didn't perform well with Resort 1, we are skipping it and moving straight to Random Forest Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest, Resort 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# #Split into test and train X and Y - labled 2 for resort 2\n",
    "# X2 = df_2.drop(columns=['TOTAL_VISITS'])\n",
    "# y2 = df_2['TOTAL_VISITS']\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# # column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['IS_HOLIDAY', 'day_of_week_num', 'Month', 'IS_SCHOOL_OUT']\n",
    "# non_cat = ['M_TEMPAVERAGE_F', 'M_PRECIP_INCHES', 'T_PRCP', 'T_TAVG',]\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "rfr_resort2 = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "rfr_pipeline_resort2 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('rfr', rfr_resort2)])\n",
    "\n",
    "#set up grid search paramater grid\n",
    "rfr_param_grid = {\n",
    "    'rfr__n_estimators': [10, 12, 14, 16, 18, 20, 22],\n",
    "    'rfr__max_depth': [None, 2, 3, 4, 5, 6, 8],\n",
    "\t'rfr__max_features': ['auto', 'sqrt', 'log2'],\n",
    "\t'rfr__criterion': ['mse', 'mae']\n",
    "}\n",
    "\n",
    "#set up gridsearch:\n",
    "gridsearch_RFR_resort2 = GridSearchCV(estimator=rfr_pipeline_resort2,\n",
    "\t\t\t\t\t\t  param_grid=rfr_param_grid,\n",
    "\t\t\t\t\t\t  cv=3)\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch_RFR_resort2.fit(X_train2, y_train2)\n",
    "\n",
    "\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "print(gridsearch_RFR_resort2.best_params_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameter Combination Found During Grid Search:\n",
      "{'rfr__criterion': 'mse', 'rfr__max_depth': None, 'rfr__max_features': 'sqrt', 'rfr__n_estimators': 20}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#pipeline_3:\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "# Pipeline second Step: treeregressor\n",
    "rfr_resort2 = RandomForestRegressor(criterion = 'mse', max_depth = None, max_features = 'sqrt', n_estimators = 20, random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_RFR_resort2 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('dtr', rfr_resort2)])\n",
    "\n",
    "pipeline_RFR_resort2.fit(X_train2, y_train2)\n",
    "\n",
    "cv_RFR_resort2 = cross_validate(pipeline_RFR_resort2, X_train2, y_train2, return_train_score=True, cv=5)\n",
    "cv_score_RFR_resort2 = cross_val_score(pipeline_RFR_resort2, X_train2, y_train2, cv = 5)\n",
    "print(f'Cross Validation Scores best Random Forest Regressor, Resort 2: {cv_RFR_resort2}')\n",
    "\n",
    "print(f'mean CV Score Random Forest Regressor, Resort 2: {cv_score_RFR_resort2.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Scores best Random Forest Regressor, Resort 2: {'fit_time': array([0.118999  , 0.07402587, 0.05672407, 0.0606389 , 0.08700633]), 'score_time': array([0.01687503, 0.01024818, 0.00751996, 0.00781393, 0.01392174]), 'test_score': array([0.23347332, 0.17485327, 0.34279815, 0.52848325, 0.47884643]), 'train_score': array([0.9124275 , 0.91811556, 0.90745626, 0.90425414, 0.9032015 ])}\n",
      "mean CV Score Random Forest Regressor, Resort 2: 0.35169088447881425\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Resort 2 we are getting dissapointing results with Random Forests at a CV score of .352\n",
    "\n",
    "Linear Regression is still the best model at CV score of.372"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting Resort 2: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#pipeline_4:\n",
    "\n",
    "# #Split into test and train X and Y - labled 2 for resort 2\n",
    "# X2 = df_2.drop(columns=['TOTAL_VISITS'])\n",
    "# y2 = df_2['TOTAL_VISITS']\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "#column transformer - this sets up simmilar to pipeline (scaler and transformer)\n",
    "\n",
    "# cat = ['IS_HOLIDAY', 'day_of_week_num', 'Month', 'IS_SCHOOL_OUT']\n",
    "# non_cat = ['M_TEMPAVERAGE_F', 'M_PRECIP_INCHES', 'T_PRCP', 'T_TAVG',]\n",
    "\n",
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "GBR_resort2 = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_GBR_resort2 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('gbr', GBR_resort2)])\n",
    "\n",
    "#set up grid search paramater grid\n",
    "gbr_param_grid = {\n",
    "    'gbr__n_estimators': [50, 100, 200, 300],\n",
    "    'gbr__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "\t'gbr__min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "#set up gridsearch:\n",
    "gridsearch_GBR_resort2 = GridSearchCV(estimator=pipeline_GBR_resort2,\n",
    "\t\t\t\t\t\t  param_grid=gbr_param_grid,\n",
    "\t\t\t\t\t\t  cv=5)\n",
    "\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch_GBR_resort2.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Best Parameter Combination Found During Gradient Boosting Grid Search:\")\n",
    "print(gridsearch_GBR_resort2.best_params_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameter Combination Found During Gradient Boosting Grid Search:\n",
      "{'gbr__criterion': 'mse', 'gbr__min_samples_split': 3, 'gbr__n_estimators': 100}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "# GBR = GradientBoostingRegressor(criterion='mse', n_estimators=200, random_state = 42)\n",
    "GBR_resort2 = GradientBoostingRegressor(criterion='mse', n_estimators=100, min_samples_split= 3, random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_gbr_resort2 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('gbr_resort2', GBR_resort2)])\n",
    "\n",
    "pipeline_gbr_resort2.fit(X_train2, y_train2)\n",
    "\n",
    "\n",
    "cv_GBR_resort2 = cross_validate(pipeline_gbr_resort2, X_train2, y_train2, return_train_score=True, cv=2)\n",
    "cv_score_GBR_resort2 = cross_val_score(pipeline_gbr_resort2, X_train2, y_train2, cv = 2)\n",
    "print(f'Cross Validation Score Gradient Boosting Model, resort 2: {cv_GBR_resort2}')\n",
    "\n",
    "print(f'mean CV Score Gradient Boosting Model, Resort 2: {cv_score_GBR_resort2.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Score Gradient Boosting Model, resort 2: {'fit_time': array([0.07868218, 0.04807401]), 'score_time': array([0.00459385, 0.00411582]), 'test_score': array([0.21149079, 0.28310764]), 'train_score': array([0.87328365, 0.85150726])}\n",
      "mean CV Score Gradient Boosting Model, Resort 2: 0.2472992146294175\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Still can't beat Linear Regression. Resort two Gradient Boosting Model returned a cv of 0.247"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resort 2 Discussion\n",
    "\n",
    "maybe could combine Saturday and Sunday into weekend vs non-weekend? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resort 3 Modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#Split into test and train X and Y\n",
    "X3 = df_3.drop(columns=['TOTAL_VISITS'])\n",
    "y3 = df_3['TOTAL_VISITS']\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "X3.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            IS_HOLIDAY  SCHOOL_OUT  M_PRCP  M_SNWD  M_TAVG  T_AWND  T_PRCP  \\\n",
       "Date                                                                         \n",
       "2015-11-21           0           0     0.0      12      36    4.92    0.00   \n",
       "2015-11-27           0           1     0.0      11      35    6.26    0.00   \n",
       "2015-11-28           0           0     0.0      11      38    9.17    0.00   \n",
       "2015-11-29           0           0     0.0      11      39   12.30    0.00   \n",
       "2015-12-04           0           0     0.9      18      30    9.84    0.26   \n",
       "\n",
       "            T_SNOW  T_TAVG  day_of_week_num  Month  \n",
       "Date                                                \n",
       "2015-11-21     0.0      44                5     11  \n",
       "2015-11-27     0.0      36                4     11  \n",
       "2015-11-28     0.0      38                5     11  \n",
       "2015-11-29     0.0      36                6     11  \n",
       "2015-12-04     0.0      47                4     12  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IS_HOLIDAY</th>\n",
       "      <th>SCHOOL_OUT</th>\n",
       "      <th>M_PRCP</th>\n",
       "      <th>M_SNWD</th>\n",
       "      <th>M_TAVG</th>\n",
       "      <th>T_AWND</th>\n",
       "      <th>T_PRCP</th>\n",
       "      <th>T_SNOW</th>\n",
       "      <th>T_TAVG</th>\n",
       "      <th>day_of_week_num</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-11-21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>12.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#take a peak at target for Resort 3: \n",
    "#visualization of target - Visits\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(y3, bins=100);"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#Establish categorical vs non-categorical for Resort 3:\n",
    "\n",
    "cat3 = ['IS_HOLIDAY', 'SCHOOL_OUT', 'day_of_week_num', 'Month']\n",
    "non_cat3 = ['M_PRCP', 'M_SNWD', 'M_TAVG', 'T_AWND', 'T_PRCP', 'T_SNOW', 'T_TAVG']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resort 3 Baseline Model: Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat3),\n",
    "\t('scaler', StandardScaler(), non_cat3),\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#setting up the pipe and training the model: \n",
    "\n",
    "LR_resort3 = LinearRegression()\n",
    "\n",
    "LRPipe_resort3 = Pipeline([('trans', Transformer),\n",
    "                ('linreg_3', LR_resort3)])\n",
    "\n",
    "LRPipe_resort3.fit(X_train3, y_train3)\n",
    "\n",
    "#cross validation and scoring\n",
    "cv_resort3 = cross_validate(LRPipe_resort3, X_train3, y_train3, return_train_score=True, cv=5)\n",
    "cv_score_resort3 = cross_val_score(LRPipe_resort3, X_train3, y_train3, cv = 5)\n",
    "\n",
    "print(f'Cross Validation Scores, Linear Regression Resort 3: {cv_resort3}')\n",
    "\n",
    "print(f'mean CV Score, Linear Regression, Resort 3: {cv_score_resort3.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Scores, Linear Regression Resort 3: {'fit_time': array([0.04591608, 0.0387733 , 0.03699493, 0.02754593, 0.02274299]), 'score_time': array([0.02709317, 0.01405883, 0.01064396, 0.01336288, 0.01113319]), 'test_score': array([0.73442309, 0.68680334, 0.56572863, 0.6350925 , 0.60191432]), 'train_score': array([0.65846221, 0.67246198, 0.70186861, 0.68290785, 0.68913727])}\n",
      "mean CV Score, Linear Regression, Resort 3: 0.6447923776236449\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resort 3 is off to a good start with a CV score of 0.645\n",
    "\n",
    "## Decision Tree Regressor (DTR) - Resort 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "Transformer = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat3),\n",
    "\t# ('scaler', StandardScaler(), non_cat3), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "DTR_resort3 = DecisionTreeRegressor(random_state = 42)\n",
    "\n",
    "#pipeline\n",
    "DTR_pipeline_resort3 = Pipeline([('transformer', Transformer),\n",
    "\t\t\t\t\t\t('dtr', DTR_resort3)])\n",
    "\n",
    "#set up grid search paramater grid\n",
    "dt_param_grid = {\n",
    "\t'dtr__splitter': ['best', 'random'],\n",
    "    'dtr__min_samples_split': [ 6, 8, 10, 12, 14, 16],\n",
    "    'dtr__max_depth': [ 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "}\n",
    "\n",
    "#set up gridsearch:\n",
    "gridsearch_DTR_resort3 = GridSearchCV(estimator=DTR_pipeline_resort3,\n",
    "\t\t\t\t\t\t  param_grid=dt_param_grid,\n",
    "\t\t\t\t\t\t  cv=3)\n",
    "\n",
    "\n",
    "#fit gridsearch\n",
    "gridsearch_DTR_resort3.fit(X_train3, y_train3)\n",
    "\n",
    "\n",
    "print(\"Best Parameter Combination Found During DRT Grid Search, Resort3:\")\n",
    "print(gridsearch_DTR_resort3.best_params_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Parameter Combination Found During DRT Grid Search, Resort3:\n",
      "{'dtr__max_depth': 9, 'dtr__min_samples_split': 14, 'dtr__splitter': 'random'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#instantiate a DTR with best params, fit model and run cross validation scoring:\n",
    "\n",
    "Transformer3 = ColumnTransformer([\n",
    "\t('ohe', OneHotEncoder(), cat3),\n",
    "\t# ('scaler', StandardScaler(), non_cat), #no need to scale on Trees\n",
    "\t], remainder ='passthrough')\n",
    "\n",
    "#Pipeline second Step: treeregressor\n",
    "DTR_resort3_best = DecisionTreeRegressor(max_depth=9, min_samples_split=14, splitter='random', random_state= 42)\n",
    "\n",
    "#pipeline\n",
    "pipeline_DTR_resort3_best = Pipeline([('transformer', Transformer3),\n",
    "\t\t\t\t\t\t('dtr_best_r3', DTR_resort3_best)])\n",
    "\n",
    "\n",
    "pipeline_DTR_resort3_best.fit(X_train3, y_train3)\n",
    "\n",
    "#cross validation and scoring\n",
    "cv_DTR_resort3 = cross_validate(gridsearch_DTR_resort3, X_train3, y_train3, return_train_score=True, cv=3)\n",
    "cv_score_DTR_resort3 = cross_val_score(gridsearch_DTR_resort3, X_train3, y_train3, cv = 3)\n",
    "\n",
    "print(f'Cross Validation Scores, DTR, Resort 3: {cv_DTR_resort3}')\n",
    "\n",
    "print(f'mean CV Score, DTR, Resort 3: {cv_score_DTR_resort3.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Scores, DTR, Resort 3: {'fit_time': array([5.6295917 , 4.44097805, 4.45834208]), 'score_time': array([0.00439334, 0.00659609, 0.00633597]), 'test_score': array([0.61144268, 0.56875645, 0.40311561]), 'train_score': array([0.74147315, 0.80259764, 0.81874819])}\n",
      "mean CV Score, DTR, Resort 3: 0.5277715790233293\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this dataset, forests are performing better than trees with a CV score of .68!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c15d3b161d9e31d3c14101c293414707f40e59d6cae0bbd6b708ca3d1e942f6f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}